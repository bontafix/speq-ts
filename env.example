# ==============================================================================
# БАЗА ДАННЫХ (PostgreSQL)
# ==============================================================================

PGHOST=localhost
PGPORT=5432
PGUSER=postgres
PGPASSWORD=your_password
PGDATABASE=equipment_catalog

# ==============================================================================
# LLM ПРОВАЙДЕРЫ
# ==============================================================================

# Выбор провайдера для chat completion (парсинг запросов, генерация ответов)
# Варианты: ollama | groq | openai
LLM_CHAT_PROVIDER=ollama

# Выбор провайдера для embeddings (векторный поиск)
# Варианты: ollama | openai
# Примечание: Groq пока не поддерживает embeddings
LLM_EMBEDDINGS_PROVIDER=ollama

# Fallback провайдеры (через запятую, в порядке приоритета)
# Если основной провайдер недоступен, будет использован первый доступный из списка
LLM_FALLBACK_PROVIDERS=ollama,groq,openai

# Тихий режим fallback (не логировать переключение провайдеров)
LLM_FALLBACK_SILENT=false

# ==============================================================================
# OLLAMA (локальный сервер)
# ==============================================================================

OLLAMA_BASE_URL=http://127.0.0.1:11434

# Модель для chat completion (парсинг запросов)
# Рекомендуемые модели:
# - qwen2.5:7b-instruct-q4_K_M (хороший баланс качества и скорости)
# - llama3.2:3b (быстрая, но меньше параметров)
# - llama3.3:70b (очень качественная, но требует много ресурсов)
LLM_MODEL=qwen2.5:7b-instruct-q4_K_M

# Модель для embeddings (векторный поиск)
# Рекомендуемые модели:
# - nomic-embed-text (384 или 768 измерений, хорошее качество)
# - all-minilm (быстрая, компактная)
EMBED_MODEL=nomic-embed-text

# Размер батча для worker эмбеддингов
EMBED_BATCH_SIZE=32

# ==============================================================================
# GROQ (облачный API)
# ==============================================================================

# API ключ Groq (получить на https://console.groq.com)
# Оставьте пустым, если не используете Groq
GROQ_API_KEY=

# Base URL для Groq API (обычно не нужно менять)
# Важно: для groq-sdk base URL должен быть БЕЗ суффикса "/openai/v1"
# (SDK сам добавляет этот префикс).
GROQ_BASE_URL=https://api.groq.com

# Рекомендуемые модели Groq:
# При использовании Groq, установите:
# LLM_CHAT_PROVIDER=groq
# LLM_MODEL=llama-3.3-70b-versatile  (очень быстрая и качественная, рекомендуется)
# или
# LLM_MODEL=llama-3.1-70b-versatile  (альтернатива)
# или
# LLM_MODEL=llama-3.1-8b-instant  (быстрая, компактная)
# или
# LLM_MODEL=mixtral-8x22b-instruct  (Mixtral MoE, большой контекст)
# или
# LLM_MODEL=gemma2-9b-it  (компактная модель Google)
#
# Получить список всех доступных моделей можно через API:
# GET https://api.groq.com/openai/v1/models (требует API ключ)

# ==============================================================================
# OPENAI (облачный API)
# ==============================================================================

# API ключ OpenAI (получить на https://platform.openai.com/api-keys)
# Оставьте пустым, если не используете OpenAI
OPENAI_API_KEY=

# Base URL для OpenAI API (обычно не нужно менять)
OPENAI_BASE_URL=https://api.openai.com/v1

# Рекомендуемые модели OpenAI:
# При использовании OpenAI, установите:
# LLM_CHAT_PROVIDER=openai
# LLM_MODEL=gpt-4o-mini  (быстрая и дешевая)
# или
# LLM_MODEL=gpt-4o  (самое высокое качество)
# или
# LLM_MODEL=gpt-3.5-turbo  (дешевая, но менее точная)

# Для embeddings:
# LLM_EMBEDDINGS_PROVIDER=openai
# EMBED_MODEL=text-embedding-3-small  (дешевая)
# или
# EMBED_MODEL=text-embedding-3-large  (высокое качество)

# ==============================================================================
# ПОИСК
# ==============================================================================

# Включить векторный поиск (pgvector)
# Требует наличия функции equipment_vector_search в БД
ENABLE_VECTOR_SEARCH=false

# ==============================================================================
# HTTP API
# ==============================================================================

HTTP_PORT=3000

# ==============================================================================
# ПРИМЕРЫ КОНФИГУРАЦИЙ
# ==============================================================================

# 1. Полностью локальная разработка (бесплатно, без интернета):
#    LLM_CHAT_PROVIDER=ollama
#    LLM_EMBEDDINGS_PROVIDER=ollama
#    LLM_MODEL=qwen2.5:7b-instruct-q4_K_M
#    EMBED_MODEL=nomic-embed-text

# 2. Быстрый парсинг через Groq + локальные embeddings (почти бесплатно):
#    LLM_CHAT_PROVIDER=groq
#    LLM_EMBEDDINGS_PROVIDER=ollama
#    LLM_MODEL=llama-3.3-70b-versatile
#    EMBED_MODEL=nomic-embed-text
#    GROQ_API_KEY=your_groq_key

# 3. Максимальное качество через OpenAI (платно):
#    LLM_CHAT_PROVIDER=openai
#    LLM_EMBEDDINGS_PROVIDER=openai
#    LLM_MODEL=gpt-4o
#    EMBED_MODEL=text-embedding-3-large
#    OPENAI_API_KEY=your_openai_key

# 4. Groq с fallback на Ollama (надежность):
#    LLM_CHAT_PROVIDER=groq
#    LLM_FALLBACK_PROVIDERS=ollama
#    GROQ_API_KEY=your_groq_key

